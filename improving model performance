## Improving model performance
## vficarrotta 2024

## Dir
setwd('')

## Libs
library('caret')

## Data
credit <- read.csv('credit.csv')

## Train a C5.0 decision tree model using caret
RNGversion('3.5.2')
set.seed(300)
m <- train(default ~ ., data=credit, method='C5.0')
# str(m)
m # condensed summary

## testing on training dataset
## Application on new data will perform worse
p <- predict(m, credit)
table(p, credit$default)

head(predict(m, credit, type='prob'))

## Automate the modeling process
ctrl <- trainControl(method='cv', number=10, selectionFunction='oneSE')

grid <- expand.grid(model='tree',
                    trials=c(1, seq(from=5, to=35, by=5)),
                    winnow=F)

RNGversion('3.5.2')
set.seed(300)

m <- train(default ~ ., data=credit, method='C5.0',
            metric='Kappa',
            trControl=ctrl,
            tuneGrid=grid)
m

# best model is 1 trial, winnow=F and tree algorithm

## Bagging (bootstrap aggregating)
library(ipred)

RNGversion('3.5.2')
set.seed(300)

mybag <- bagging(as.factor(default) ~., data=credit, nbagg=25)

credit_pred <- predict(mybag, credit)
table(credit_pred, credit$default)

## Train tree model using caret and method=treebag
RNGversion('3.5.2')
set.seed(300)

ctrl <- trainControl(method='cv', number=10)
train(default ~ ., data=credit, method='treebag', trControl=ctrl)

## bagged tree model performs slightly better than the best C5.0 decision tree

## Boosting
library(adabag)
library(vcd)

credit <- read.csv('credit.csv', stringsAsFactors=T)

RNGversion('3.5.2')
set.seed(300)
m_adaboost <- boosting(default ~ ., data=credit)
p_adaboost <- predict(m_adaboost, credit)

head(p_adaboost$class)

p_adaboost$confusion

RNGversion('3.5.2')
set.seed(300)
adaboost_cv <- boosting.cv(default ~ ., data=credit)

adaboost_cv$confusion
Kappa(adaboost_cv$confusion)

## Random forests
library(randomForest)

RNGversion('3.5.2')
set.seed(300)
rf <- randomForest(default ~ ., data=credit)

Kappa(rf$confusion[1:2, 1:2])

## Compare the autotuned boosted C5.0 to autotuned random forest

ctrl <- trainControl(method='repeatedcv',
                    number=10,
                    repeats=10,
                    savePredictions=T,
                    classProbs=T,
                    summaryFunction=twoClassSummary)

grid_rf <- expand.grid(mtry=c(2,4,8,16))

RNGversion('3.5.2')
set.seed(300)

m_rf <- train(default~., data=credit, method='rf', metric='ROC', trControl=ctrl, tuneGrid=grid_rf)

## compare models
grid_c50 <- expand.grid(model='tree',
                        trials=c(10, 25, 50, 100,
                        winnow=F))
RNGversion('3.5.2')
set.seed(300)
m_c50 <- train(default ~ ., data=credit, method='C5.0', metric='ROC', trControl=ctrl, tuneGrid=grid_c50)

## visualize comparison
library(pROC)
roc_rf <- roc(m_rf$pred$obs, m_rf$pred$yes)
roc_c50 <- roc(m_c50$pred$obs, m_c50$pred$yes)
plot(roc_rf, col='red', legacy.axes=T)
plot(roc_c50, col='blue', add=T)

# Larger aread under the curve score indicates better model
